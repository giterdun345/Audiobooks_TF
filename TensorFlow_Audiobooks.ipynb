{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data\n",
    "raw_df= pd.read_csv('Audiobooks_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14084 entries, 0 to 14083\n",
      "Data columns (total 12 columns):\n",
      "ID                  14084 non-null int64\n",
      "overall_minutes     14084 non-null float64\n",
      "avg_minutes         14084 non-null int64\n",
      "price_overall       14084 non-null float64\n",
      "price_avg           14084 non-null float64\n",
      "review              14084 non-null int64\n",
      "review_scale        2468 non-null float64\n",
      "completion          14084 non-null float64\n",
      "minutes_listened    14084 non-null float64\n",
      "support_request     14084 non-null int64\n",
      "days_between        14084 non-null int64\n",
      "convert             14084 non-null int64\n",
      "dtypes: float64(6), int64(6)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting index as ID and fill NaN with zero\n",
    "raw_df.set_index('ID', inplace = True)\n",
    "raw_df.review_scale.fillna(0, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_minutes</th>\n",
       "      <th>avg_minutes</th>\n",
       "      <th>price_overall</th>\n",
       "      <th>price_avg</th>\n",
       "      <th>review</th>\n",
       "      <th>review_scale</th>\n",
       "      <th>completion</th>\n",
       "      <th>minutes_listened</th>\n",
       "      <th>support_request</th>\n",
       "      <th>days_between</th>\n",
       "      <th>convert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "      <td>14084.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1591.281685</td>\n",
       "      <td>1678.608634</td>\n",
       "      <td>7.103791</td>\n",
       "      <td>7.543805</td>\n",
       "      <td>0.160750</td>\n",
       "      <td>1.561132</td>\n",
       "      <td>0.125659</td>\n",
       "      <td>189.888983</td>\n",
       "      <td>0.070222</td>\n",
       "      <td>61.935033</td>\n",
       "      <td>0.158833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>504.340663</td>\n",
       "      <td>654.838599</td>\n",
       "      <td>4.931673</td>\n",
       "      <td>5.560129</td>\n",
       "      <td>0.367313</td>\n",
       "      <td>3.447537</td>\n",
       "      <td>0.241206</td>\n",
       "      <td>371.084010</td>\n",
       "      <td>0.472157</td>\n",
       "      <td>88.207634</td>\n",
       "      <td>0.365533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>216.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1188.000000</td>\n",
       "      <td>1188.000000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1620.000000</td>\n",
       "      <td>1620.000000</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>6.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>194.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2160.000000</td>\n",
       "      <td>7020.000000</td>\n",
       "      <td>130.940000</td>\n",
       "      <td>130.940000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall_minutes   avg_minutes  price_overall     price_avg  \\\n",
       "count     14084.000000  14084.000000   14084.000000  14084.000000   \n",
       "mean       1591.281685   1678.608634       7.103791      7.543805   \n",
       "std         504.340663    654.838599       4.931673      5.560129   \n",
       "min         216.000000    216.000000       3.860000      3.860000   \n",
       "25%        1188.000000   1188.000000       5.330000      5.330000   \n",
       "50%        1620.000000   1620.000000       5.950000      6.070000   \n",
       "75%        2160.000000   2160.000000       8.000000      8.000000   \n",
       "max        2160.000000   7020.000000     130.940000    130.940000   \n",
       "\n",
       "             review  review_scale    completion  minutes_listened  \\\n",
       "count  14084.000000  14084.000000  14084.000000      14084.000000   \n",
       "mean       0.160750      1.561132      0.125659        189.888983   \n",
       "std        0.367313      3.447537      0.241206        371.084010   \n",
       "min        0.000000      0.000000      0.000000          0.000000   \n",
       "25%        0.000000      0.000000      0.000000          0.000000   \n",
       "50%        0.000000      0.000000      0.000000          0.000000   \n",
       "75%        0.000000      0.000000      0.130000        194.400000   \n",
       "max        1.000000     10.000000      1.000000       2160.000000   \n",
       "\n",
       "       support_request  days_between       convert  \n",
       "count     14084.000000  14084.000000  14084.000000  \n",
       "mean          0.070222     61.935033      0.158833  \n",
       "std           0.472157     88.207634      0.365533  \n",
       "min           0.000000      0.000000      0.000000  \n",
       "25%           0.000000      0.000000      0.000000  \n",
       "50%           0.000000     11.000000      0.000000  \n",
       "75%           0.000000    105.000000      0.000000  \n",
       "max          30.000000    464.000000      1.000000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering/ selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average price per minute to exacerbate the value\n",
    "raw_df['price_per_minute'] = raw_df.price_avg / raw_df.avg_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rating is 8.91, instead of replacing no ratings with mean, any mean greater than 8 will be assigned 1\n",
    "raw_df['review'] = [1 if rating > 8 else 0 for rating in raw_df['review_scale']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the difference between the two and eliminate the average minutes column \n",
    "raw_df['minutes_difference'] = raw_df.overall_minutes - raw_df.avg_minutes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the difference between the two and eliminate the average price column \n",
    "raw_df['price_difference'] = raw_df.price_overall - raw_df.price_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop review scale column due to missing data; alternatively we could \n",
    "raw_df.drop(['avg_minutes', 'price_avg', 'review_scale',], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14084 entries, 994 to 251\n",
      "Data columns (total 11 columns):\n",
      "overall_minutes       14084 non-null float64\n",
      "price_overall         14084 non-null float64\n",
      "review                14084 non-null int64\n",
      "completion            14084 non-null float64\n",
      "minutes_listened      14084 non-null float64\n",
      "support_request       14084 non-null int64\n",
      "days_between          14084 non-null int64\n",
      "convert               14084 non-null int64\n",
      "price_per_minute      14084 non-null float64\n",
      "minutes_difference    14084 non-null float64\n",
      "price_difference      14084 non-null float64\n",
      "dtypes: float64(7), int64(4)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it; first shuffle\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_df = raw_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating the independent and dependant variables to oversample\n",
    "inputs = shuffled_df.drop('convert', axis = 1)\n",
    "targets = shuffled_df.convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is 23694\n",
      "Number of no conversion 11847\n",
      "Number of conversions 11847\n",
      "Proportion of no conversion  0.5\n",
      "Proportion of conversion 0.5\n"
     ]
    }
   ],
   "source": [
    "# balance data for better results from sampling \n",
    "smote = SMOTE(random_state = 13)\n",
    "smote_X, smote_y = smote.fit_sample(inputs, targets)\n",
    "X = pd.DataFrame(smote_X, columns = inputs.columns )\n",
    "y= pd.DataFrame(smote_y, columns=['convert'])\n",
    "# # we can Check the numbers of our data\n",
    "print(\"length is\",len(smote_X))\n",
    "print(\"Number of no conversion\",len(y[y['convert'] == 0]))\n",
    "print(\"Number of conversions\",len(y[y['convert'] == 1]))\n",
    "print(\"Proportion of no conversion \", len(y[y['convert'] == 0]) / len(X))\n",
    "print(\"Proportion of conversion\", len(y[y['convert'] == 1]) / len(X))\n",
    "# class is balanced with equal proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling input data can try Minmax, quantile transform for better results\n",
    "scaler = StandardScaler()\n",
    "inputs = scaler.fit_transform(X)\n",
    "inputs_df = pd.DataFrame(inputs, columns = X.columns)\n",
    "# create one df to seperate into train, val and test npz files\n",
    "df = inputs_df.merge(y, left_index = True, right_index = True)\n",
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23693, 23694)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test\n",
    "# Count the total number of samples\n",
    "samples_count = df.shape[0]\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "test_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# check \n",
    "train_samples_count + validation_samples_count + test_samples_count, samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets Obs Ratio\n",
      "9402.0 18955 0.5\n",
      "1217.0 2369 0.51\n",
      "1228.0 2369 0.52\n"
     ]
    }
   ],
   "source": [
    "# second shuffle to ensure randomness\n",
    "df = df.sample(frac = 1)\n",
    "\n",
    "# Create variables that record the inputs and targets for training.\n",
    "train_inputs = df.iloc[:train_samples_count ]\n",
    "train_targets =  df.convert.iloc[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "val_inputs = df.iloc[train_samples_count : train_samples_count + validation_samples_count]\n",
    "val_targets = df.convert.iloc[train_samples_count : train_samples_count + validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = df.iloc[train_samples_count + validation_samples_count:]\n",
    "test_targets = df.convert.iloc[train_samples_count + validation_samples_count:]\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print('Targets', 'Obs', 'Ratio')\n",
    "print(np.sum(train_targets), train_samples_count, round(np.sum(train_targets) / train_samples_count, 2))\n",
    "print(np.sum(val_targets), validation_samples_count, round(np.sum(val_targets) / validation_samples_count, 2))\n",
    "print(np.sum(test_targets), test_samples_count, round(np.sum(test_targets) / test_samples_count, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the three datasets in *.npz. for future use\n",
    "np.savez('Audiobooks_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_validation', inputs=val_inputs, targets=val_targets)\n",
    "np.savez('Audiobooks_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that will do the batching for the algorithm\n",
    "# This code is extremely reusable. You should just change Audiobooks_data everywhere in the code\n",
    "class Audiobooks_Data_Reader():\n",
    "    # Dataset is a mandatory arugment, while the batch_size is optional\n",
    "    # If you don't input batch_size, it will automatically take the value: None\n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "    \n",
    "        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n",
    "        # e.g. if I call this class with x('train',5), it will load 'Audiobooks_data_train.npz' with a batch size of 5.\n",
    "        npz = np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        # Two variables that take the values of the inputs and the targets. Inputs are floats, targets are integers\n",
    "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "        \n",
    "        # Counts the batch number, given the size you feed it later\n",
    "        # If the batch size is None, we are either validating or testing, so we want to take the data in a single batch\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    # A method which loads the next batch\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "            \n",
    "        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "        \n",
    "        # One-hot encode the targets. In this example it's a bit superfluous since we have a 0/1 column \n",
    "        # as a target already but we're giving you the code regardless, as it will be useful for any \n",
    "        # classification task with more than one target column\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        # The function will return the inputs batch and the one-hot encoded targets\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "        \n",
    "    # A method needed for iterating over the batches, as we will put them in a loop\n",
    "    # This tells Python that the class we're defining is iterable, i.e. that we can use it like:\n",
    "    # for input, output in data: \n",
    "        # do things\n",
    "    # An iterator in Python is a class with a method __next__ that defines exactly how to iterate through its objects\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.728. Validation loss: 0.638. Validation accuracy: 69.13%. Hidden_layers:350\n",
      "Epoch 2. Training loss: 0.611. Validation loss: 0.577. Validation accuracy: 74.94%. Hidden_layers:350\n",
      "Epoch 3. Training loss: 0.563. Validation loss: 0.537. Validation accuracy: 74.05%. Hidden_layers:350\n",
      "Epoch 4. Training loss: 0.529. Validation loss: 0.507. Validation accuracy: 75.62%. Hidden_layers:350\n",
      "Epoch 5. Training loss: 0.502. Validation loss: 0.483. Validation accuracy: 75.39%. Hidden_layers:350\n",
      "Epoch 6. Training loss: 0.481. Validation loss: 0.465. Validation accuracy: 75.39%. Hidden_layers:350\n",
      "Epoch 7. Training loss: 0.464. Validation loss: 0.450. Validation accuracy: 75.84%. Hidden_layers:350\n",
      "Epoch 8. Training loss: 0.450. Validation loss: 0.438. Validation accuracy: 75.62%. Hidden_layers:350\n",
      "Epoch 9. Training loss: 0.439. Validation loss: 0.428. Validation accuracy: 75.62%. Hidden_layers:350\n",
      "Epoch 10. Training loss: 0.429. Validation loss: 0.420. Validation accuracy: 75.17%. Hidden_layers:350\n",
      "Epoch 11. Training loss: 0.421. Validation loss: 0.413. Validation accuracy: 75.62%. Hidden_layers:350\n",
      "Epoch 12. Training loss: 0.414. Validation loss: 0.407. Validation accuracy: 76.73%. Hidden_layers:350\n",
      "Epoch 13. Training loss: 0.407. Validation loss: 0.402. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 14. Training loss: 0.402. Validation loss: 0.397. Validation accuracy: 76.29%. Hidden_layers:350\n",
      "Epoch 15. Training loss: 0.397. Validation loss: 0.393. Validation accuracy: 76.51%. Hidden_layers:350\n",
      "Epoch 16. Training loss: 0.392. Validation loss: 0.389. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 17. Training loss: 0.388. Validation loss: 0.386. Validation accuracy: 77.40%. Hidden_layers:350\n",
      "Epoch 18. Training loss: 0.384. Validation loss: 0.383. Validation accuracy: 77.40%. Hidden_layers:350\n",
      "Epoch 19. Training loss: 0.381. Validation loss: 0.380. Validation accuracy: 77.18%. Hidden_layers:350\n",
      "Epoch 20. Training loss: 0.378. Validation loss: 0.378. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 21. Training loss: 0.375. Validation loss: 0.375. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 22. Training loss: 0.372. Validation loss: 0.373. Validation accuracy: 77.18%. Hidden_layers:350\n",
      "Epoch 23. Training loss: 0.370. Validation loss: 0.371. Validation accuracy: 77.63%. Hidden_layers:350\n",
      "Epoch 24. Training loss: 0.367. Validation loss: 0.370. Validation accuracy: 77.40%. Hidden_layers:350\n",
      "Epoch 25. Training loss: 0.365. Validation loss: 0.368. Validation accuracy: 76.51%. Hidden_layers:350\n",
      "Epoch 26. Training loss: 0.363. Validation loss: 0.367. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 27. Training loss: 0.361. Validation loss: 0.365. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 28. Training loss: 0.360. Validation loss: 0.364. Validation accuracy: 76.96%. Hidden_layers:350\n",
      "Epoch 29. Training loss: 0.358. Validation loss: 0.363. Validation accuracy: 77.18%. Hidden_layers:350\n",
      "Epoch 30. Training loss: 0.356. Validation loss: 0.362. Validation accuracy: 77.40%. Hidden_layers:350\n",
      "Epoch 31. Training loss: 0.355. Validation loss: 0.361. Validation accuracy: 77.63%. Hidden_layers:350\n",
      "Epoch 32. Training loss: 0.353. Validation loss: 0.360. Validation accuracy: 78.08%. Hidden_layers:350\n",
      "Epoch 33. Training loss: 0.352. Validation loss: 0.359. Validation accuracy: 78.08%. Hidden_layers:350\n",
      "Epoch 34. Training loss: 0.351. Validation loss: 0.358. Validation accuracy: 78.30%. Hidden_layers:350\n",
      "Epoch 35. Training loss: 0.350. Validation loss: 0.358. Validation accuracy: 78.08%. Hidden_layers:350\n",
      "Epoch 36. Training loss: 0.348. Validation loss: 0.357. Validation accuracy: 78.30%. Hidden_layers:350\n",
      "Epoch 37. Training loss: 0.347. Validation loss: 0.356. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 38. Training loss: 0.346. Validation loss: 0.356. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 39. Training loss: 0.345. Validation loss: 0.355. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 40. Training loss: 0.344. Validation loss: 0.354. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 41. Training loss: 0.343. Validation loss: 0.354. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 42. Training loss: 0.343. Validation loss: 0.353. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 43. Training loss: 0.342. Validation loss: 0.353. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 44. Training loss: 0.341. Validation loss: 0.352. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 45. Training loss: 0.340. Validation loss: 0.352. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 46. Training loss: 0.339. Validation loss: 0.352. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 47. Training loss: 0.339. Validation loss: 0.351. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 48. Training loss: 0.338. Validation loss: 0.351. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 49. Training loss: 0.337. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 50. Training loss: 0.337. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 51. Training loss: 0.336. Validation loss: 0.350. Validation accuracy: 78.97%. Hidden_layers:350\n",
      "Epoch 52. Training loss: 0.335. Validation loss: 0.349. Validation accuracy: 78.97%. Hidden_layers:350\n",
      "Epoch 53. Training loss: 0.335. Validation loss: 0.349. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 54. Training loss: 0.334. Validation loss: 0.349. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 55. Training loss: 0.334. Validation loss: 0.348. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 56. Training loss: 0.333. Validation loss: 0.348. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 57. Training loss: 0.333. Validation loss: 0.348. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 58. Training loss: 0.332. Validation loss: 0.347. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 59. Training loss: 0.332. Validation loss: 0.347. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 60. Training loss: 0.331. Validation loss: 0.347. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 61. Training loss: 0.331. Validation loss: 0.346. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 62. Training loss: 0.330. Validation loss: 0.346. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 63. Training loss: 0.330. Validation loss: 0.346. Validation accuracy: 78.52%. Hidden_layers:350\n",
      "Epoch 64. Training loss: 0.329. Validation loss: 0.346. Validation accuracy: 78.97%. Hidden_layers:350\n",
      "Epoch 65. Training loss: 0.329. Validation loss: 0.345. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 66. Training loss: 0.329. Validation loss: 0.345. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 67. Training loss: 0.328. Validation loss: 0.345. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 68. Training loss: 0.328. Validation loss: 0.344. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 69. Training loss: 0.327. Validation loss: 0.344. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 70. Training loss: 0.327. Validation loss: 0.344. Validation accuracy: 78.75%. Hidden_layers:350\n",
      "Epoch 71. Training loss: 0.327. Validation loss: 0.344. Validation accuracy: 78.97%. Hidden_layers:350\n",
      "Epoch 72. Training loss: 0.326. Validation loss: 0.343. Validation accuracy: 79.19%. Hidden_layers:350\n",
      "Epoch 73. Training loss: 0.326. Validation loss: 0.343. Validation accuracy: 79.19%. Hidden_layers:350\n",
      "Epoch 74. Training loss: 0.326. Validation loss: 0.343. Validation accuracy: 79.19%. Hidden_layers:350\n",
      "Epoch 75. Training loss: 0.325. Validation loss: 0.343. Validation accuracy: 79.19%. Hidden_layers:350\n",
      "Epoch 76. Training loss: 0.325. Validation loss: 0.342. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 77. Training loss: 0.325. Validation loss: 0.342. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 78. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 79. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 80. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.64%. Hidden_layers:350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81. Training loss: 0.323. Validation loss: 0.341. Validation accuracy: 79.64%. Hidden_layers:350\n",
      "Epoch 82. Training loss: 0.323. Validation loss: 0.341. Validation accuracy: 79.64%. Hidden_layers:350\n",
      "Epoch 83. Training loss: 0.323. Validation loss: 0.341. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 84. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 85. Training loss: 0.322. Validation loss: 0.340. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 86. Training loss: 0.322. Validation loss: 0.340. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 87. Training loss: 0.322. Validation loss: 0.340. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 88. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 89. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 90. Training loss: 0.321. Validation loss: 0.339. Validation accuracy: 79.42%. Hidden_layers:350\n",
      "Epoch 91. Training loss: 0.321. Validation loss: 0.339. Validation accuracy: 79.64%. Hidden_layers:350\n",
      "Epoch 92. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 79.87%. Hidden_layers:350\n",
      "Epoch 93. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 80.09%. Hidden_layers:350\n",
      "Epoch 94. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 80.09%. Hidden_layers:350\n",
      "Epoch 95. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:350\n",
      "Epoch 96. Training loss: 0.319. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:350\n",
      "Epoch 97. Training loss: 0.319. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:350\n",
      "Epoch 98. Training loss: 0.319. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:350\n",
      "Epoch 99. Training loss: 0.319. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:350\n",
      "Epoch 100. Training loss: 0.319. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:350\n",
      "End of training.\n",
      "Epoch 1. Training loss: 0.732. Validation loss: 0.635. Validation accuracy: 61.07%. Hidden_layers:450\n",
      "Epoch 2. Training loss: 0.607. Validation loss: 0.572. Validation accuracy: 69.57%. Hidden_layers:450\n",
      "Epoch 3. Training loss: 0.554. Validation loss: 0.528. Validation accuracy: 73.83%. Hidden_layers:450\n",
      "Epoch 4. Training loss: 0.517. Validation loss: 0.496. Validation accuracy: 74.50%. Hidden_layers:450\n",
      "Epoch 5. Training loss: 0.489. Validation loss: 0.472. Validation accuracy: 76.29%. Hidden_layers:450\n",
      "Epoch 6. Training loss: 0.467. Validation loss: 0.454. Validation accuracy: 75.39%. Hidden_layers:450\n",
      "Epoch 7. Training loss: 0.451. Validation loss: 0.440. Validation accuracy: 75.39%. Hidden_layers:450\n",
      "Epoch 8. Training loss: 0.437. Validation loss: 0.428. Validation accuracy: 76.96%. Hidden_layers:450\n",
      "Epoch 9. Training loss: 0.426. Validation loss: 0.418. Validation accuracy: 76.51%. Hidden_layers:450\n",
      "Epoch 10. Training loss: 0.416. Validation loss: 0.411. Validation accuracy: 76.96%. Hidden_layers:450\n",
      "Epoch 11. Training loss: 0.408. Validation loss: 0.404. Validation accuracy: 77.18%. Hidden_layers:450\n",
      "Epoch 12. Training loss: 0.402. Validation loss: 0.398. Validation accuracy: 77.40%. Hidden_layers:450\n",
      "Epoch 13. Training loss: 0.396. Validation loss: 0.393. Validation accuracy: 77.18%. Hidden_layers:450\n",
      "Epoch 14. Training loss: 0.390. Validation loss: 0.389. Validation accuracy: 77.18%. Hidden_layers:450\n",
      "Epoch 15. Training loss: 0.386. Validation loss: 0.385. Validation accuracy: 76.73%. Hidden_layers:450\n",
      "Epoch 16. Training loss: 0.381. Validation loss: 0.382. Validation accuracy: 76.96%. Hidden_layers:450\n",
      "Epoch 17. Training loss: 0.377. Validation loss: 0.379. Validation accuracy: 76.96%. Hidden_layers:450\n",
      "Epoch 18. Training loss: 0.374. Validation loss: 0.376. Validation accuracy: 76.73%. Hidden_layers:450\n",
      "Epoch 19. Training loss: 0.371. Validation loss: 0.374. Validation accuracy: 77.18%. Hidden_layers:450\n",
      "Epoch 20. Training loss: 0.368. Validation loss: 0.371. Validation accuracy: 77.40%. Hidden_layers:450\n",
      "Epoch 21. Training loss: 0.365. Validation loss: 0.370. Validation accuracy: 77.63%. Hidden_layers:450\n",
      "Epoch 22. Training loss: 0.363. Validation loss: 0.368. Validation accuracy: 78.08%. Hidden_layers:450\n",
      "Epoch 23. Training loss: 0.361. Validation loss: 0.366. Validation accuracy: 78.52%. Hidden_layers:450\n",
      "Epoch 24. Training loss: 0.359. Validation loss: 0.365. Validation accuracy: 78.52%. Hidden_layers:450\n",
      "Epoch 25. Training loss: 0.357. Validation loss: 0.364. Validation accuracy: 78.52%. Hidden_layers:450\n",
      "Epoch 26. Training loss: 0.355. Validation loss: 0.362. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 27. Training loss: 0.353. Validation loss: 0.361. Validation accuracy: 78.52%. Hidden_layers:450\n",
      "Epoch 28. Training loss: 0.352. Validation loss: 0.360. Validation accuracy: 78.52%. Hidden_layers:450\n",
      "Epoch 29. Training loss: 0.350. Validation loss: 0.359. Validation accuracy: 78.52%. Hidden_layers:450\n",
      "Epoch 30. Training loss: 0.349. Validation loss: 0.359. Validation accuracy: 78.30%. Hidden_layers:450\n",
      "Epoch 31. Training loss: 0.347. Validation loss: 0.358. Validation accuracy: 78.30%. Hidden_layers:450\n",
      "Epoch 32. Training loss: 0.346. Validation loss: 0.357. Validation accuracy: 78.30%. Hidden_layers:450\n",
      "Epoch 33. Training loss: 0.345. Validation loss: 0.356. Validation accuracy: 78.30%. Hidden_layers:450\n",
      "Epoch 34. Training loss: 0.344. Validation loss: 0.355. Validation accuracy: 78.30%. Hidden_layers:450\n",
      "Epoch 35. Training loss: 0.343. Validation loss: 0.355. Validation accuracy: 78.30%. Hidden_layers:450\n",
      "Epoch 36. Training loss: 0.342. Validation loss: 0.354. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 37. Training loss: 0.341. Validation loss: 0.354. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 38. Training loss: 0.340. Validation loss: 0.353. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 39. Training loss: 0.339. Validation loss: 0.352. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 40. Training loss: 0.338. Validation loss: 0.352. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 41. Training loss: 0.337. Validation loss: 0.351. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 42. Training loss: 0.337. Validation loss: 0.351. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 43. Training loss: 0.336. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 44. Training loss: 0.335. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 45. Training loss: 0.334. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 46. Training loss: 0.334. Validation loss: 0.349. Validation accuracy: 78.75%. Hidden_layers:450\n",
      "Epoch 47. Training loss: 0.333. Validation loss: 0.349. Validation accuracy: 78.97%. Hidden_layers:450\n",
      "Epoch 48. Training loss: 0.332. Validation loss: 0.348. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 49. Training loss: 0.332. Validation loss: 0.348. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 50. Training loss: 0.331. Validation loss: 0.348. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 51. Training loss: 0.331. Validation loss: 0.347. Validation accuracy: 79.19%. Hidden_layers:450\n",
      "Epoch 52. Training loss: 0.330. Validation loss: 0.347. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 53. Training loss: 0.330. Validation loss: 0.346. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 54. Training loss: 0.329. Validation loss: 0.346. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 55. Training loss: 0.329. Validation loss: 0.346. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 56. Training loss: 0.328. Validation loss: 0.345. Validation accuracy: 79.64%. Hidden_layers:450\n",
      "Epoch 57. Training loss: 0.328. Validation loss: 0.345. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 58. Training loss: 0.327. Validation loss: 0.345. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 59. Training loss: 0.327. Validation loss: 0.344. Validation accuracy: 79.42%. Hidden_layers:450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60. Training loss: 0.326. Validation loss: 0.344. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 61. Training loss: 0.326. Validation loss: 0.344. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 62. Training loss: 0.325. Validation loss: 0.343. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 63. Training loss: 0.325. Validation loss: 0.343. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 64. Training loss: 0.324. Validation loss: 0.343. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 65. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 66. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.64%. Hidden_layers:450\n",
      "Epoch 67. Training loss: 0.323. Validation loss: 0.342. Validation accuracy: 79.42%. Hidden_layers:450\n",
      "Epoch 68. Training loss: 0.323. Validation loss: 0.342. Validation accuracy: 79.64%. Hidden_layers:450\n",
      "Epoch 69. Training loss: 0.323. Validation loss: 0.341. Validation accuracy: 79.64%. Hidden_layers:450\n",
      "Epoch 70. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 79.87%. Hidden_layers:450\n",
      "Epoch 71. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 79.87%. Hidden_layers:450\n",
      "Epoch 72. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 73. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 74. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 75. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 76. Training loss: 0.320. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 77. Training loss: 0.320. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 78. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 79. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 80. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 81. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 82. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 83. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 84. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 85. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 86. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 87. Training loss: 0.317. Validation loss: 0.338. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 88. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 89. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 90. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 91. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 80.09%. Hidden_layers:450\n",
      "Epoch 92. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 93. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 94. Training loss: 0.316. Validation loss: 0.336. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 95. Training loss: 0.316. Validation loss: 0.336. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 96. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.31%. Hidden_layers:450\n",
      "Epoch 97. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:450\n",
      "Epoch 98. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:450\n",
      "Epoch 99. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:450\n",
      "Epoch 100. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:450\n",
      "End of training.\n",
      "Epoch 1. Training loss: 0.704. Validation loss: 0.618. Validation accuracy: 69.13%. Hidden_layers:500\n",
      "Epoch 2. Training loss: 0.589. Validation loss: 0.555. Validation accuracy: 73.60%. Hidden_layers:500\n",
      "Epoch 3. Training loss: 0.542. Validation loss: 0.515. Validation accuracy: 74.72%. Hidden_layers:500\n",
      "Epoch 4. Training loss: 0.507. Validation loss: 0.485. Validation accuracy: 75.17%. Hidden_layers:500\n",
      "Epoch 5. Training loss: 0.480. Validation loss: 0.463. Validation accuracy: 76.29%. Hidden_layers:500\n",
      "Epoch 6. Training loss: 0.460. Validation loss: 0.446. Validation accuracy: 75.84%. Hidden_layers:500\n",
      "Epoch 7. Training loss: 0.444. Validation loss: 0.432. Validation accuracy: 76.73%. Hidden_layers:500\n",
      "Epoch 8. Training loss: 0.432. Validation loss: 0.422. Validation accuracy: 76.06%. Hidden_layers:500\n",
      "Epoch 9. Training loss: 0.421. Validation loss: 0.413. Validation accuracy: 75.84%. Hidden_layers:500\n",
      "Epoch 10. Training loss: 0.412. Validation loss: 0.406. Validation accuracy: 75.84%. Hidden_layers:500\n",
      "Epoch 11. Training loss: 0.405. Validation loss: 0.400. Validation accuracy: 76.51%. Hidden_layers:500\n",
      "Epoch 12. Training loss: 0.398. Validation loss: 0.394. Validation accuracy: 76.29%. Hidden_layers:500\n",
      "Epoch 13. Training loss: 0.392. Validation loss: 0.390. Validation accuracy: 76.51%. Hidden_layers:500\n",
      "Epoch 14. Training loss: 0.387. Validation loss: 0.386. Validation accuracy: 76.96%. Hidden_layers:500\n",
      "Epoch 15. Training loss: 0.383. Validation loss: 0.382. Validation accuracy: 76.96%. Hidden_layers:500\n",
      "Epoch 16. Training loss: 0.379. Validation loss: 0.379. Validation accuracy: 76.73%. Hidden_layers:500\n",
      "Epoch 17. Training loss: 0.375. Validation loss: 0.376. Validation accuracy: 77.18%. Hidden_layers:500\n",
      "Epoch 18. Training loss: 0.371. Validation loss: 0.374. Validation accuracy: 77.40%. Hidden_layers:500\n",
      "Epoch 19. Training loss: 0.368. Validation loss: 0.371. Validation accuracy: 77.40%. Hidden_layers:500\n",
      "Epoch 20. Training loss: 0.365. Validation loss: 0.369. Validation accuracy: 76.96%. Hidden_layers:500\n",
      "Epoch 21. Training loss: 0.363. Validation loss: 0.367. Validation accuracy: 77.63%. Hidden_layers:500\n",
      "Epoch 22. Training loss: 0.360. Validation loss: 0.366. Validation accuracy: 77.85%. Hidden_layers:500\n",
      "Epoch 23. Training loss: 0.358. Validation loss: 0.364. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 24. Training loss: 0.356. Validation loss: 0.363. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 25. Training loss: 0.354. Validation loss: 0.362. Validation accuracy: 77.85%. Hidden_layers:500\n",
      "Epoch 26. Training loss: 0.352. Validation loss: 0.361. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 27. Training loss: 0.351. Validation loss: 0.359. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 28. Training loss: 0.349. Validation loss: 0.358. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 29. Training loss: 0.348. Validation loss: 0.358. Validation accuracy: 78.30%. Hidden_layers:500\n",
      "Epoch 30. Training loss: 0.346. Validation loss: 0.357. Validation accuracy: 78.30%. Hidden_layers:500\n",
      "Epoch 31. Training loss: 0.345. Validation loss: 0.356. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 32. Training loss: 0.344. Validation loss: 0.355. Validation accuracy: 78.08%. Hidden_layers:500\n",
      "Epoch 33. Training loss: 0.343. Validation loss: 0.354. Validation accuracy: 78.30%. Hidden_layers:500\n",
      "Epoch 34. Training loss: 0.342. Validation loss: 0.354. Validation accuracy: 78.30%. Hidden_layers:500\n",
      "Epoch 35. Training loss: 0.341. Validation loss: 0.353. Validation accuracy: 78.30%. Hidden_layers:500\n",
      "Epoch 36. Training loss: 0.340. Validation loss: 0.353. Validation accuracy: 78.75%. Hidden_layers:500\n",
      "Epoch 37. Training loss: 0.339. Validation loss: 0.352. Validation accuracy: 78.97%. Hidden_layers:500\n",
      "Epoch 38. Training loss: 0.338. Validation loss: 0.351. Validation accuracy: 78.97%. Hidden_layers:500\n",
      "Epoch 39. Training loss: 0.337. Validation loss: 0.351. Validation accuracy: 78.97%. Hidden_layers:500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Training loss: 0.336. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:500\n",
      "Epoch 41. Training loss: 0.335. Validation loss: 0.350. Validation accuracy: 78.75%. Hidden_layers:500\n",
      "Epoch 42. Training loss: 0.335. Validation loss: 0.349. Validation accuracy: 78.75%. Hidden_layers:500\n",
      "Epoch 43. Training loss: 0.334. Validation loss: 0.349. Validation accuracy: 78.75%. Hidden_layers:500\n",
      "Epoch 44. Training loss: 0.333. Validation loss: 0.349. Validation accuracy: 78.75%. Hidden_layers:500\n",
      "Epoch 45. Training loss: 0.333. Validation loss: 0.348. Validation accuracy: 79.19%. Hidden_layers:500\n",
      "Epoch 46. Training loss: 0.332. Validation loss: 0.348. Validation accuracy: 79.19%. Hidden_layers:500\n",
      "Epoch 47. Training loss: 0.331. Validation loss: 0.347. Validation accuracy: 79.42%. Hidden_layers:500\n",
      "Epoch 48. Training loss: 0.331. Validation loss: 0.347. Validation accuracy: 79.64%. Hidden_layers:500\n",
      "Epoch 49. Training loss: 0.330. Validation loss: 0.346. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 50. Training loss: 0.330. Validation loss: 0.346. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 51. Training loss: 0.329. Validation loss: 0.346. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 52. Training loss: 0.328. Validation loss: 0.345. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 53. Training loss: 0.328. Validation loss: 0.345. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 54. Training loss: 0.327. Validation loss: 0.345. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 55. Training loss: 0.327. Validation loss: 0.344. Validation accuracy: 80.31%. Hidden_layers:500\n",
      "Epoch 56. Training loss: 0.326. Validation loss: 0.344. Validation accuracy: 80.31%. Hidden_layers:500\n",
      "Epoch 57. Training loss: 0.326. Validation loss: 0.344. Validation accuracy: 80.31%. Hidden_layers:500\n",
      "Epoch 58. Training loss: 0.326. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 59. Training loss: 0.325. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 60. Training loss: 0.325. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 61. Training loss: 0.324. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 62. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 63. Training loss: 0.324. Validation loss: 0.342. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 64. Training loss: 0.323. Validation loss: 0.342. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 65. Training loss: 0.323. Validation loss: 0.342. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 66. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 67. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 68. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 69. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 70. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 80.09%. Hidden_layers:500\n",
      "Epoch 71. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 72. Training loss: 0.320. Validation loss: 0.340. Validation accuracy: 79.87%. Hidden_layers:500\n",
      "Epoch 73. Training loss: 0.320. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:500\n",
      "Epoch 74. Training loss: 0.320. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:500\n",
      "Epoch 75. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.31%. Hidden_layers:500\n",
      "Epoch 76. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.54%. Hidden_layers:500\n",
      "Epoch 77. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.54%. Hidden_layers:500\n",
      "Epoch 78. Training loss: 0.319. Validation loss: 0.338. Validation accuracy: 80.54%. Hidden_layers:500\n",
      "Epoch 79. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.54%. Hidden_layers:500\n",
      "Epoch 80. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.54%. Hidden_layers:500\n",
      "Epoch 81. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 82. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 83. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 84. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 85. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 86. Training loss: 0.317. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 87. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 88. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 89. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 90. Training loss: 0.316. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 91. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 92. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 93. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.76%. Hidden_layers:500\n",
      "Epoch 94. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "Epoch 95. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "Epoch 96. Training loss: 0.314. Validation loss: 0.335. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "Epoch 97. Training loss: 0.314. Validation loss: 0.335. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "Epoch 98. Training loss: 0.314. Validation loss: 0.335. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "Epoch 99. Training loss: 0.314. Validation loss: 0.335. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "Epoch 100. Training loss: 0.314. Validation loss: 0.335. Validation accuracy: 80.98%. Hidden_layers:500\n",
      "End of training.\n",
      "Epoch 1. Training loss: 0.654. Validation loss: 0.580. Validation accuracy: 72.26%. Hidden_layers:700\n",
      "Epoch 2. Training loss: 0.553. Validation loss: 0.512. Validation accuracy: 76.06%. Hidden_layers:700\n",
      "Epoch 3. Training loss: 0.502. Validation loss: 0.472. Validation accuracy: 76.51%. Hidden_layers:700\n",
      "Epoch 4. Training loss: 0.468. Validation loss: 0.446. Validation accuracy: 76.06%. Hidden_layers:700\n",
      "Epoch 5. Training loss: 0.445. Validation loss: 0.429. Validation accuracy: 75.84%. Hidden_layers:700\n",
      "Epoch 6. Training loss: 0.429. Validation loss: 0.416. Validation accuracy: 75.39%. Hidden_layers:700\n",
      "Epoch 7. Training loss: 0.416. Validation loss: 0.406. Validation accuracy: 75.84%. Hidden_layers:700\n",
      "Epoch 8. Training loss: 0.405. Validation loss: 0.398. Validation accuracy: 76.06%. Hidden_layers:700\n",
      "Epoch 9. Training loss: 0.397. Validation loss: 0.391. Validation accuracy: 75.62%. Hidden_layers:700\n",
      "Epoch 10. Training loss: 0.390. Validation loss: 0.386. Validation accuracy: 76.06%. Hidden_layers:700\n",
      "Epoch 11. Training loss: 0.384. Validation loss: 0.382. Validation accuracy: 76.73%. Hidden_layers:700\n",
      "Epoch 12. Training loss: 0.379. Validation loss: 0.378. Validation accuracy: 76.96%. Hidden_layers:700\n",
      "Epoch 13. Training loss: 0.374. Validation loss: 0.374. Validation accuracy: 77.18%. Hidden_layers:700\n",
      "Epoch 14. Training loss: 0.370. Validation loss: 0.372. Validation accuracy: 77.18%. Hidden_layers:700\n",
      "Epoch 15. Training loss: 0.366. Validation loss: 0.369. Validation accuracy: 77.63%. Hidden_layers:700\n",
      "Epoch 16. Training loss: 0.363. Validation loss: 0.367. Validation accuracy: 77.85%. Hidden_layers:700\n",
      "Epoch 17. Training loss: 0.360. Validation loss: 0.365. Validation accuracy: 77.85%. Hidden_layers:700\n",
      "Epoch 18. Training loss: 0.358. Validation loss: 0.364. Validation accuracy: 78.08%. Hidden_layers:700\n",
      "Epoch 19. Training loss: 0.355. Validation loss: 0.362. Validation accuracy: 78.08%. Hidden_layers:700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Training loss: 0.353. Validation loss: 0.361. Validation accuracy: 78.30%. Hidden_layers:700\n",
      "Epoch 21. Training loss: 0.351. Validation loss: 0.360. Validation accuracy: 78.30%. Hidden_layers:700\n",
      "Epoch 22. Training loss: 0.349. Validation loss: 0.359. Validation accuracy: 78.30%. Hidden_layers:700\n",
      "Epoch 23. Training loss: 0.347. Validation loss: 0.358. Validation accuracy: 78.30%. Hidden_layers:700\n",
      "Epoch 24. Training loss: 0.346. Validation loss: 0.357. Validation accuracy: 78.30%. Hidden_layers:700\n",
      "Epoch 25. Training loss: 0.344. Validation loss: 0.356. Validation accuracy: 78.52%. Hidden_layers:700\n",
      "Epoch 26. Training loss: 0.343. Validation loss: 0.355. Validation accuracy: 78.75%. Hidden_layers:700\n",
      "Epoch 27. Training loss: 0.342. Validation loss: 0.354. Validation accuracy: 78.97%. Hidden_layers:700\n",
      "Epoch 28. Training loss: 0.340. Validation loss: 0.354. Validation accuracy: 79.19%. Hidden_layers:700\n",
      "Epoch 29. Training loss: 0.339. Validation loss: 0.353. Validation accuracy: 78.97%. Hidden_layers:700\n",
      "Epoch 30. Training loss: 0.338. Validation loss: 0.352. Validation accuracy: 78.97%. Hidden_layers:700\n",
      "Epoch 31. Training loss: 0.337. Validation loss: 0.352. Validation accuracy: 79.19%. Hidden_layers:700\n",
      "Epoch 32. Training loss: 0.336. Validation loss: 0.351. Validation accuracy: 79.19%. Hidden_layers:700\n",
      "Epoch 33. Training loss: 0.335. Validation loss: 0.350. Validation accuracy: 79.19%. Hidden_layers:700\n",
      "Epoch 34. Training loss: 0.334. Validation loss: 0.350. Validation accuracy: 79.19%. Hidden_layers:700\n",
      "Epoch 35. Training loss: 0.334. Validation loss: 0.349. Validation accuracy: 79.19%. Hidden_layers:700\n",
      "Epoch 36. Training loss: 0.333. Validation loss: 0.349. Validation accuracy: 79.42%. Hidden_layers:700\n",
      "Epoch 37. Training loss: 0.332. Validation loss: 0.348. Validation accuracy: 79.42%. Hidden_layers:700\n",
      "Epoch 38. Training loss: 0.331. Validation loss: 0.348. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 39. Training loss: 0.331. Validation loss: 0.347. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 40. Training loss: 0.330. Validation loss: 0.347. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 41. Training loss: 0.329. Validation loss: 0.346. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 42. Training loss: 0.329. Validation loss: 0.346. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 43. Training loss: 0.328. Validation loss: 0.346. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 44. Training loss: 0.327. Validation loss: 0.345. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 45. Training loss: 0.327. Validation loss: 0.345. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 46. Training loss: 0.326. Validation loss: 0.344. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 47. Training loss: 0.326. Validation loss: 0.344. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 48. Training loss: 0.325. Validation loss: 0.344. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 49. Training loss: 0.325. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 50. Training loss: 0.324. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 51. Training loss: 0.324. Validation loss: 0.343. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 52. Training loss: 0.323. Validation loss: 0.342. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 53. Training loss: 0.323. Validation loss: 0.342. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 54. Training loss: 0.322. Validation loss: 0.342. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 55. Training loss: 0.322. Validation loss: 0.341. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 56. Training loss: 0.321. Validation loss: 0.341. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 57. Training loss: 0.321. Validation loss: 0.341. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 58. Training loss: 0.321. Validation loss: 0.340. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 59. Training loss: 0.320. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 60. Training loss: 0.320. Validation loss: 0.340. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 61. Training loss: 0.319. Validation loss: 0.340. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 62. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 63. Training loss: 0.319. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 64. Training loss: 0.318. Validation loss: 0.339. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 65. Training loss: 0.318. Validation loss: 0.339. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 66. Training loss: 0.318. Validation loss: 0.338. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 67. Training loss: 0.317. Validation loss: 0.338. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 68. Training loss: 0.317. Validation loss: 0.338. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 69. Training loss: 0.317. Validation loss: 0.338. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 70. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 71. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 72. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 73. Training loss: 0.316. Validation loss: 0.337. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 74. Training loss: 0.315. Validation loss: 0.337. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 75. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 76. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 77. Training loss: 0.315. Validation loss: 0.336. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 78. Training loss: 0.314. Validation loss: 0.336. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 79. Training loss: 0.314. Validation loss: 0.336. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 80. Training loss: 0.314. Validation loss: 0.336. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 81. Training loss: 0.314. Validation loss: 0.335. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 82. Training loss: 0.313. Validation loss: 0.335. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 83. Training loss: 0.313. Validation loss: 0.335. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 84. Training loss: 0.313. Validation loss: 0.335. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 85. Training loss: 0.313. Validation loss: 0.335. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 86. Training loss: 0.313. Validation loss: 0.335. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 87. Training loss: 0.312. Validation loss: 0.334. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 88. Training loss: 0.312. Validation loss: 0.334. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 89. Training loss: 0.312. Validation loss: 0.334. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 90. Training loss: 0.312. Validation loss: 0.334. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 91. Training loss: 0.312. Validation loss: 0.334. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 92. Training loss: 0.311. Validation loss: 0.334. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 93. Training loss: 0.311. Validation loss: 0.334. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 94. Training loss: 0.311. Validation loss: 0.334. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "Epoch 95. Training loss: 0.311. Validation loss: 0.333. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 96. Training loss: 0.311. Validation loss: 0.333. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 97. Training loss: 0.310. Validation loss: 0.333. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 98. Training loss: 0.310. Validation loss: 0.333. Validation accuracy: 81.21%. Hidden_layers:700\n",
      "Epoch 99. Training loss: 0.310. Validation loss: 0.333. Validation accuracy: 81.21%. Hidden_layers:700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100. Training loss: 0.310. Validation loss: 0.333. Validation accuracy: 81.43%. Hidden_layers:700\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "# Input size depends on the number of input variables. \n",
    "input_size = 10\n",
    "# Output size is 2, as we one-hot encoded the targets.\n",
    "output_size = 2\n",
    "# Choose a hidden_layer_size\n",
    "hidden_layer = [350, 450, 500, 700]\n",
    "for hidden_layer_size in hidden_layer:\n",
    "    # Reset the default graph, so you can fiddle with the hyperparameters and then rerun the code.\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Create the placeholders\n",
    "    inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "    targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "    # Outline the model. We will create a net with 2 hidden layers\n",
    "    weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "    biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "    outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "    weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "    biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "    outputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "    weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "    biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "    # We will incorporate the softmax activation into the loss, as in the previous example\n",
    "    outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "    # Use the softmax cross entropy loss with logits\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Get a 0 or 1 for every input indicating whether it output the correct answer\n",
    "    out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "    # Optimize with Adam\n",
    "    optimize = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(mean_loss)\n",
    "\n",
    "    # Create a session\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Initialize the variables\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "\n",
    "    # Choose the batch size\n",
    "    batch_size = 100\n",
    "\n",
    "    # Set early stopping mechanisms\n",
    "    max_epochs = 100\n",
    "    prev_validation_loss = 9999999.\n",
    "\n",
    "    # Load the first batch of training and validation, using the class we created. \n",
    "    # Arguments are ending of 'Audiobooks_Data_<...>', where for <...> we input 'train', 'validation', or 'test'\n",
    "    # depending on what we want to load\n",
    "    train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "    validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "    # Create the loop for epochs \n",
    "    for epoch_counter in range(max_epochs):\n",
    "\n",
    "        # Set the epoch loss to 0, and make it a float\n",
    "        curr_epoch_loss = 0.\n",
    "\n",
    "        # Iterate over the training data \n",
    "        # Since train_data is an instance of the Audiobooks_Data_Reader class,\n",
    "        # we can iterate through it by implicitly using the __next__ method we defined above.\n",
    "        # As a reminder, it batches samples together, one-hot encodes the targets, and returns\n",
    "        # inputs and targets batch by batch\n",
    "        for input_batch, target_batch in train_data:\n",
    "            _, batch_loss = sess.run([optimize, mean_loss], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            #Record the batch loss into the current epoch loss\n",
    "            curr_epoch_loss += batch_loss\n",
    "\n",
    "        # Find the mean curr_epoch_loss\n",
    "        # batch_count is a variable, defined in the Audiobooks_Data_Reader class\n",
    "        curr_epoch_loss /= train_data.batch_count\n",
    "\n",
    "        # Set validation loss and accuracy for the epoch to zero\n",
    "        validation_loss = 0.\n",
    "        validation_accuracy = 0.\n",
    "\n",
    "        # Use the same logic of the code to forward propagate the validation set\n",
    "        # There will be a single batch, as the class was created in this way\n",
    "        for input_batch, target_batch in validation_data:\n",
    "            validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "        # Print statistics for the current epoch\n",
    "        print('Epoch '+str(epoch_counter+1)+\n",
    "              '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "              '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "              '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%. '+\n",
    "             'Hidden_layers:' '{0}'.format(hidden_layer_size))\n",
    "\n",
    "        # Trigger early stopping if validation loss begins increasing.\n",
    "        if validation_loss > prev_validation_loss:\n",
    "            break\n",
    "\n",
    "        # Store this epoch's validation loss to be used as previous in the next iteration.\n",
    "        prev_validation_loss = validation_loss\n",
    "\n",
    "    print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 83.93%\n"
     ]
    }
   ],
   "source": [
    "# Load the test data, following the same logic as we did for the train_data and validation data\n",
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "# Forward propagate through the training set. This time we only need the accuracy\n",
    "for inputs_batch, targets_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy],\n",
    "                     feed_dict={inputs: inputs_batch, targets: targets_batch})\n",
    "\n",
    "# Get the test accuracy in percentages\n",
    "# When sess.run is has a single output, we get a list (that's how it was coded by Google), rather than a float.\n",
    "# Therefore, we must take the first value from the list (the value at position 0)\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.714. Validation loss: 0.676. Validation accuracy: 53.47%. Hidden_layers:700\n",
      "Epoch 2. Training loss: 0.669. Validation loss: 0.651. Validation accuracy: 68.46%. Hidden_layers:700\n",
      "Epoch 3. Training loss: 0.637. Validation loss: 0.605. Validation accuracy: 75.84%. Hidden_layers:700\n",
      "Epoch 4. Training loss: 0.582. Validation loss: 0.537. Validation accuracy: 76.06%. Hidden_layers:700\n",
      "Epoch 5. Training loss: 0.517. Validation loss: 0.475. Validation accuracy: 75.39%. Hidden_layers:700\n",
      "Epoch 6. Training loss: 0.464. Validation loss: 0.435. Validation accuracy: 76.29%. Hidden_layers:700\n",
      "Epoch 7. Training loss: 0.431. Validation loss: 0.412. Validation accuracy: 75.84%. Hidden_layers:700\n",
      "Epoch 8. Training loss: 0.410. Validation loss: 0.397. Validation accuracy: 76.51%. Hidden_layers:700\n",
      "Epoch 9. Training loss: 0.395. Validation loss: 0.387. Validation accuracy: 77.63%. Hidden_layers:700\n",
      "Epoch 10. Training loss: 0.385. Validation loss: 0.380. Validation accuracy: 78.08%. Hidden_layers:700\n",
      "Epoch 11. Training loss: 0.376. Validation loss: 0.374. Validation accuracy: 78.75%. Hidden_layers:700\n",
      "Epoch 12. Training loss: 0.369. Validation loss: 0.370. Validation accuracy: 78.97%. Hidden_layers:700\n",
      "Epoch 13. Training loss: 0.364. Validation loss: 0.366. Validation accuracy: 79.42%. Hidden_layers:700\n",
      "Epoch 14. Training loss: 0.359. Validation loss: 0.364. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 15. Training loss: 0.355. Validation loss: 0.361. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 16. Training loss: 0.352. Validation loss: 0.360. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 17. Training loss: 0.349. Validation loss: 0.358. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 18. Training loss: 0.346. Validation loss: 0.357. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 19. Training loss: 0.344. Validation loss: 0.356. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 20. Training loss: 0.342. Validation loss: 0.355. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 21. Training loss: 0.340. Validation loss: 0.354. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 22. Training loss: 0.338. Validation loss: 0.354. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 23. Training loss: 0.337. Validation loss: 0.353. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 24. Training loss: 0.335. Validation loss: 0.352. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 25. Training loss: 0.334. Validation loss: 0.351. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 26. Training loss: 0.333. Validation loss: 0.351. Validation accuracy: 79.64%. Hidden_layers:700\n",
      "Epoch 27. Training loss: 0.332. Validation loss: 0.350. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 28. Training loss: 0.331. Validation loss: 0.350. Validation accuracy: 79.87%. Hidden_layers:700\n",
      "Epoch 29. Training loss: 0.330. Validation loss: 0.349. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 30. Training loss: 0.329. Validation loss: 0.349. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 31. Training loss: 0.328. Validation loss: 0.348. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 32. Training loss: 0.327. Validation loss: 0.347. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 33. Training loss: 0.326. Validation loss: 0.347. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 34. Training loss: 0.326. Validation loss: 0.346. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 35. Training loss: 0.325. Validation loss: 0.346. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 36. Training loss: 0.324. Validation loss: 0.346. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 37. Training loss: 0.323. Validation loss: 0.345. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 38. Training loss: 0.323. Validation loss: 0.345. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 39. Training loss: 0.322. Validation loss: 0.344. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 40. Training loss: 0.322. Validation loss: 0.344. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 41. Training loss: 0.321. Validation loss: 0.343. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 42. Training loss: 0.320. Validation loss: 0.343. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 43. Training loss: 0.320. Validation loss: 0.343. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 44. Training loss: 0.319. Validation loss: 0.342. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 45. Training loss: 0.319. Validation loss: 0.342. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 46. Training loss: 0.318. Validation loss: 0.342. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 47. Training loss: 0.318. Validation loss: 0.342. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 48. Training loss: 0.317. Validation loss: 0.341. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 49. Training loss: 0.317. Validation loss: 0.341. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 50. Training loss: 0.317. Validation loss: 0.341. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 51. Training loss: 0.316. Validation loss: 0.340. Validation accuracy: 80.31%. Hidden_layers:700\n",
      "Epoch 52. Training loss: 0.316. Validation loss: 0.340. Validation accuracy: 80.09%. Hidden_layers:700\n",
      "Epoch 53. Training loss: 0.315. Validation loss: 0.340. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 54. Training loss: 0.315. Validation loss: 0.340. Validation accuracy: 80.54%. Hidden_layers:700\n",
      "Epoch 55. Training loss: 0.315. Validation loss: 0.340. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 56. Training loss: 0.314. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 57. Training loss: 0.314. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 58. Training loss: 0.314. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 59. Training loss: 0.313. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 60. Training loss: 0.313. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 61. Training loss: 0.313. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 62. Training loss: 0.313. Validation loss: 0.339. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 63. Training loss: 0.312. Validation loss: 0.338. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 64. Training loss: 0.312. Validation loss: 0.338. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 65. Training loss: 0.312. Validation loss: 0.338. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 66. Training loss: 0.311. Validation loss: 0.338. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 67. Training loss: 0.311. Validation loss: 0.338. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 68. Training loss: 0.311. Validation loss: 0.338. Validation accuracy: 80.98%. Hidden_layers:700\n",
      "Epoch 69. Training loss: 0.311. Validation loss: 0.338. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 70. Training loss: 0.310. Validation loss: 0.338. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 71. Training loss: 0.310. Validation loss: 0.338. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 72. Training loss: 0.310. Validation loss: 0.338. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 73. Training loss: 0.310. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 74. Training loss: 0.310. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 75. Training loss: 0.309. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 76. Training loss: 0.309. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 77. Training loss: 0.309. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 78. Training loss: 0.309. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 79. Training loss: 0.309. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 80. Training loss: 0.308. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81. Training loss: 0.308. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 82. Training loss: 0.308. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 83. Training loss: 0.308. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "Epoch 84. Training loss: 0.308. Validation loss: 0.337. Validation accuracy: 80.76%. Hidden_layers:700\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.opt import NadamOptimizer\n",
    "\n",
    "# Input size depends on the number of input variables. \n",
    "input_size = 10\n",
    "# Output size is 2, as we one-hot encoded the targets.\n",
    "output_size = 2\n",
    "# Choose a hidden_layer_size\n",
    "hidden_layer = [700]\n",
    "for hidden_layer_size in hidden_layer:\n",
    "    # Reset the default graph, so you can fiddle with the hyperparameters and then rerun the code.\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Create the placeholders\n",
    "    inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "    targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "    # Outline the model. We will create a net with 2 hidden layers\n",
    "    weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "    biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "    outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "    weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "    biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "    outputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "    weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "    biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "    # We will incorporate the softmax activation into the loss, as in the previous example\n",
    "    outputs_3 = tf.nn.leaky_relu(tf.matmul(outputs_2, weights_3) + biases_3)\n",
    "\n",
    "    weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, hidden_layer_size])\n",
    "    biases_4 = tf.get_variable(\"biases_4\", [hidden_layer_size])\n",
    "    # We will incorporate the softmax activation into the loss, as in the previous example\n",
    "    outputs_4 = tf.nn.sigmoid(tf.matmul(outputs_3, weights_4) + biases_4)\n",
    "    \n",
    "    weights_5 = tf.get_variable(\"weights_5\", [hidden_layer_size, output_size])\n",
    "    biases_5 = tf.get_variable(\"biases_5\", [output_size])\n",
    "    # We will incorporate the softmax activation into the loss, as in the previous example\n",
    "    outputs = tf.matmul(outputs_4, weights_5) + biases_5\n",
    "    \n",
    "    # Use the softmax cross entropy loss with logits\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Get a 0 or 1 for every input indicating whether it output the correct answer\n",
    "    out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "    # Optimize with Adam\n",
    "    optimize = NadamOptimizer(learning_rate=0.0001).minimize(mean_loss)\n",
    "\n",
    "    # Create a session\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Initialize the variables\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "\n",
    "    # Choose the batch size\n",
    "    batch_size = 200\n",
    "\n",
    "    # Set early stopping mechanisms\n",
    "    max_epochs = 150\n",
    "    prev_validation_loss = 9999999.\n",
    "\n",
    "    # Load the first batch of training and validation, using the class we created. \n",
    "    # Arguments are ending of 'Audiobooks_Data_<...>', where for <...> we input 'train', 'validation', or 'test'\n",
    "    # depending on what we want to load\n",
    "    train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "    validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "    # Create the loop for epochs \n",
    "    for epoch_counter in range(max_epochs):\n",
    "\n",
    "        # Set the epoch loss to 0, and make it a float\n",
    "        curr_epoch_loss = 0.\n",
    "\n",
    "        # Iterate over the training data \n",
    "        # Since train_data is an instance of the Audiobooks_Data_Reader class,\n",
    "        # we can iterate through it by implicitly using the __next__ method we defined above.\n",
    "        # As a reminder, it batches samples together, one-hot encodes the targets, and returns\n",
    "        # inputs and targets batch by batch\n",
    "        for input_batch, target_batch in train_data:\n",
    "            _, batch_loss = sess.run([optimize, mean_loss], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            #Record the batch loss into the current epoch loss\n",
    "            curr_epoch_loss += batch_loss\n",
    "\n",
    "        # Find the mean curr_epoch_loss\n",
    "        # batch_count is a variable, defined in the Audiobooks_Data_Reader class\n",
    "        curr_epoch_loss /= train_data.batch_count\n",
    "\n",
    "        # Set validation loss and accuracy for the epoch to zero\n",
    "        validation_loss = 0.\n",
    "        validation_accuracy = 0.\n",
    "\n",
    "        # Use the same logic of the code to forward propagate the validation set\n",
    "        # There will be a single batch, as the class was created in this way\n",
    "        for input_batch, target_batch in validation_data:\n",
    "            validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "        # Print statistics for the current epoch\n",
    "        print('Epoch '+ str(epoch_counter + 1) +\n",
    "              '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss) +\n",
    "              '. Validation loss: '+'{0:.3f}'.format(validation_loss) +\n",
    "              '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.) + '%'+\n",
    "              '. Hidden_layers:' '{0}'.format(hidden_layer_size))\n",
    "\n",
    "        # Trigger early stopping if validation loss begins increasing.\n",
    "        if validation_loss > prev_validation_loss:\n",
    "            break\n",
    "\n",
    "        # Store this epoch's validation loss to be used as previous in the next iteration.\n",
    "        prev_validation_loss = validation_loss\n",
    "\n",
    "    print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 85.04%\n"
     ]
    }
   ],
   "source": [
    "# Load the test data, following the same logic as we did for the train_data and validation data\n",
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "# Forward propagate through the training set. This time we only need the accuracy\n",
    "for inputs_batch, targets_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy],\n",
    "                     feed_dict={inputs: inputs_batch, targets: targets_batch})\n",
    "\n",
    "# Get the test accuracy in percentages\n",
    "# When sess.run is has a single output, we get a list (that's how it was coded by Google), rather than a float.\n",
    "# Therefore, we must take the first value from the list (the value at position 0)\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
